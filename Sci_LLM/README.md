## LLMs in science
### Tasks

|   Date    |     Name     | Publication | Repositories |
| :-------: | :---------- | :--------- | :---------: |
| `2023.4` | MaterialsBERT | [A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing](https://www.nature.com/articles/s41524-023-01003-w) | |
| `2022.10` | SolvBERT | [ SolvBERT for solvation free energy and solubility prediction: a demonstration of an NLP model for predicting the properties of molecular complexes](https://chemrxiv.org/engage/chemrxiv/article-details/633d6bbfea6a225b1809e24e) | |
| `2022.9` | ChemBERTa-2 | [ChemBERTa-2: Towards Chemical Foundation Models](https://arxiv.org/abs/2209.01712) | |
| `2022.5` | MatSciBERT | [MatSciBERT: A materials domain language model for text mining and information extraction](https://www.nature.com/articles/s41524-022-00784-w) | |
| `2022.5` | MolFormer | [Molformer: Large Scale Chemical Language Representations Capture Molecular Structure and Properties](https://www.researchsquare.com/article/rs-1570270/v1) | |
| `2022.5` | ScholarBERT | [The Diminishing Returns of Masked Language Models to Science](https://arxiv.org/abs/2205.11342) | |
| `2022.4` | MatBERT | [TiMolecular representation learning with language models and domain-relevant auxiliary taskstle](https://arxiv.org/abs/2011.13230) | |
| `2021.10` | MTL-BERT | [ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments](https://arxiv.org/abs/2110.03895) | |
| `2021.10` | PubMedBERT | [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754) | |
| `2021.9` | Mol-BERT | [Generative Adversarial Networks for Multi-Modal Multimedia Computing](https://www.hindawi.com/journals/wcmc/2021/7181815/) | |
| `2021.6` | ChemBERT | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://pubs.acs.org/doi/10.1021/acs.jcim.1c00284) | |
| `2020.10` | Bio-Megatron | [BioMegatron: Larger Biomedical Domain Language Model](https://arxiv.org/abs/2010.06060) | |
| `2020.10` | ChemBERTa | [ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction](https://arxiv.org/abs/2010.09885) | |
| `2020.4` | ClinicalBERT | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) | |
| `2020.11` | MolBERT | [Molecular representation learning with language models and domain-relevant auxiliary tasks](https://arxiv.org/abs/2011.13230) | |
| `2020.12` | Schwaller2020 | [Mapping the Space of Chemical Reactions Using Attention-Based Neural Networks](https://arxiv.org/abs/2012.06051) | |
| `2019.9` | SMILES-BERT | [SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction](https://dl.acm.org/doi/10.1145/3307339.3342186) | |
| `2019.9` | SciBERT | [SciBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676) | |
| `2019.9` | BioBERT      |  [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506) | [<img src="../assets/github-mark-white.svg" width="25" />](https://github.com/naver/biobert-pretrained) | 
| `2019.6` | BlueBERT | [Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets](https://arxiv.org/abs/1906.05474) | |
| `2019.4` | ClinicalBERT |  [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) | [ðŸ¤—](https://huggingface.co/medicalai/ClinicalBERT) |
| `` |  | [Title]() | |

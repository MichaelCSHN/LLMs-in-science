## LLMs in science
### Tasks

|   Date    |     Name        | Publication | Repositories |
| :-------: | :-------------- | :---------- | :----------: |
| `2023.04` | MaterialsBERT   | [A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing](https://www.nature.com/articles/s41524-023-01003-w) | |
| `2022.10` | SolvBERT        | [ SolvBERT for solvation free energy and solubility prediction: a demonstration of an NLP model for predicting the properties of molecular complexes](https://chemrxiv.org/engage/chemrxiv/article-details/633d6bbfea6a225b1809e24e) | |
| `2022.09` | ChemBERTa-2     | [ChemBERTa-2: Towards Chemical Foundation Models](https://arxiv.org/abs/2209.01712) | |
| `2022.05` | MatSciBERT      | [MatSciBERT: A materials domain language model for text mining and information extraction](https://www.nature.com/articles/s41524-022-00784-w) | |
| `2022.05` | MolFormer       | [Molformer: Large Scale Chemical Language Representations Capture Molecular Structure and Properties](https://www.researchsquare.com/article/rs-1570270/v1) | |
| `2022.05` | ScholarBERT     | [The Diminishing Returns of Masked Language Models to Science](https://arxiv.org/abs/2205.11342) | |
| `2022.04` | MatBERT         | [TiMolecular representation learning with language models and domain-relevant auxiliary taskstle](https://arxiv.org/abs/2011.13230) | |
| `2021.10` | MTL-BERT        | [ALL-IN-ONE: Multi-Task Learning BERT models for Evaluating Peer Assessments](https://arxiv.org/abs/2110.03895) | |
| `2021.10` | PubMedBERT      | [Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing](https://dl.acm.org/doi/10.1145/3458754) | |
| `2021.09` | Mol-BERT        | [Generative Adversarial Networks for Multi-Modal Multimedia Computing](https://www.hindawi.com/journals/wcmc/2021/7181815/) | |
| `2021.06` | ChemBERT        | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://pubs.acs.org/doi/10.1021/acs.jcim.1c00284) | |
| `2020.10` | Bio-Megatron    | [BioMegatron: Larger Biomedical Domain Language Model](https://arxiv.org/abs/2010.06060) | |
| `2020.10` | ChemBERTa       | [ChemBERTa: Large-Scale Self-Supervised Pretraining for Molecular Property Prediction](https://arxiv.org/abs/2010.09885) | |
| `2020.10` | MOFTransformers | [Moftransformer: a Multi-modal Pre-training Transformer for Universal Transfer Learning in Metal-organic Frameworks](https://chemrxiv.org/engage/chemrxiv/article-details/634fbf8a4a18764f58e9fda5) | [<img src="../assets/github-mark-white.svg" width="20" />](https://github.com/hspark1212/MOFTransformer) |
| `2020.04` | ClinicalBERT    | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) | |
| `2020.11` | MolBERT         | [Molecular representation learning with language models and domain-relevant auxiliary tasks](https://arxiv.org/abs/2011.13230) | |
| `2020.12` | Schwaller2020   | [Mapping the Space of Chemical Reactions Using Attention-Based Neural Networks](https://arxiv.org/abs/2012.06051) | |
| `2019.09` | SMILES-BERT     | [SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction](https://dl.acm.org/doi/10.1145/3307339.3342186) | |
| `2019.09` | SciBERT         | [SciBERT: A Pretrained Language Model for Scientific Text](https://arxiv.org/abs/1903.10676) | |
| `2019.09` | BioBERT         | [BioBERT: a pre-trained biomedical language representation model for biomedical text mining](https://academic.oup.com/bioinformatics/article/36/4/1234/5566506) | [<img src="../assets/github-mark-white.svg" width="25" />](https://github.com/naver/biobert-pretrained) | 
| `2019.06` | BlueBERT        | [Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets](https://arxiv.org/abs/1906.05474) | |
| `2019.04` | ClinicalBERT    | [ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission](https://arxiv.org/abs/1904.05342) | [ðŸ¤—](https://huggingface.co/medicalai/ClinicalBERT) |

### Reviews
|   Date    |     Name     | Publication | Repositories |
| :-------: | :---------- | :--------- | :---------: |
| `2024.02` |  | [Advances in machine learning with chemical language models in molecular property and reaction outcome predictions](https://onlinelibrary.wiley.com/doi/10.1002/jcc.27315) | |
| `2024.01` |  | [Scientific Large Language Models: A Survey on Biological & Chemical Domains](https://arxiv.org/abs/2401.14656) | |
| `2024.01` |  | [The Future of Molecular Studies through the Lens of Large Language Models](https://pubs.acs.org/doi/10.1021/acs.jcim.3c01977) | |
| `2023.05` |  | [What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks](https://arxiv.org/abs/2305.18365) | |
| `2023.04` |  | [Chemical language models for de novo drug design: Challenges and opportunities](https://www.sciencedirect.com/science/article/pii/S0959440X23000015?via%3Dihub) | |
| `2023.03` |  | [The future of chemistry is language](https://www.nature.com/articles/s41570-023-00502-0) | |
| `2023.01` |  | [Assessment of chemistry knowledge in large language models that generate code](https://pubs.rsc.org/en/content/articlelanding/2023/dd/d2dd00087c) | |
